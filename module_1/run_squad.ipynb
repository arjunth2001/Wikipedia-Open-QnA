{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open(\"../data/dev-v2.0.json\") as f:\n",
    "    data = json.load(f)\n",
    "data = data['data']\n",
    "jsonl=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from retriever import Retriever\n",
    "from gensim.matutils import cossim\n",
    "from gensim import utils\n",
    "BASE_PATH = \"/scratch/arjunth2001/\"\n",
    "ret = Retriever(BASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027a44471a874b97ad1307851ef6b661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "corpus =[]\n",
    "for doc in tqdm(data):\n",
    "    title = doc['title']\n",
    "    paragraphs = doc['paragraphs']\n",
    "    for para in paragraphs:\n",
    "        qas = para['qas']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            context = para['context']\n",
    "            answers = [a['text'] for a in qa['answers']]\n",
    "            if (question,answers,context,title) not in corpus:\n",
    "                corpus.append((question,answers,context,title))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11872"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_length(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    doc = nlp(text, disable=['parser','tagger','ner'])\n",
    "    return len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TYPE = {'what was': 0, 'what is': 1, 'what': 2, 'in what': 3, 'in which': 4, 'in': 5,\n",
    "          'when': 6, 'where': 7, 'who': 8, 'why': 9, 'which': 10, 'is': 11, 'other': 12}\n",
    "\n",
    "\n",
    "def get_question_type_features(question):\n",
    "    qwords = question.split(' ')\n",
    "    if qwords[0].lower() in Q_TYPE:\n",
    "        return Q_TYPE[qwords[0].lower()]\n",
    "    if ' '.join(list(map(lambda x: x.lower(), qwords[0:2]))) in Q_TYPE:\n",
    "        return Q_TYPE[' '.join(list(map(lambda x: x.lower(), qwords[0:2])))]\n",
    "    return Q_TYPE['other']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "connection = sqlite3.connect(\"/scratch/arjunth2001/wiki.db\", check_same_thread=False)\n",
    "\n",
    "\n",
    "def get_doc_text(doc_id):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\n",
    "        \"SELECT text FROM documents WHERE id = ?\",\n",
    "        (doc_id,)\n",
    "    )\n",
    "    result = cursor.fetchone()\n",
    "    cursor.close()\n",
    "    if result is None:\n",
    "        return None\n",
    "    return result[0]\n",
    "def get_doc_text2(title):\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\n",
    "        \"SELECT id , text FROM documents WHERE title = ?\",\n",
    "        (title,)\n",
    "    )\n",
    "    result = cursor.fetchone()\n",
    "    cursor.close()\n",
    "    if result is None:\n",
    "        return None\n",
    "    return result[0], result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Euchlaena is a genus of moths in the family Geometridae erected by Jacob Hübner in 1823.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_text(\"23621594\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('23621594',\n",
       " 'Euchlaena is a genus of moths in the family Geometridae erected by Jacob Hübner in 1823.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_doc_text2(\"Euchlaena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11872"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4094bef3983048e492a17f08a841273c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/arjunth2001/miniconda3/envs/legal/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "file_path=\"/scratch/arjunth2001/dataset1\"\n",
    "jsonl = []\n",
    "for i,(q,ans,c,t) in tqdm(enumerate(corpus),total=len(corpus)):\n",
    "    rets = ret.get_similar(q)\n",
    "    q_vec = ret.getTfidfForText(q)\n",
    "    #_id , doc_text = get_doc_text2(t)\n",
    "    #my_dict = {\n",
    "        #'doc_id': _id,\n",
    "        #'title':t,\n",
    "        #'doc_sim': cossim(q_vec, ret.getTfidfForText(doc_text)),\n",
    "        #'par_sim': cossim(q_vec, ret.getTfidfForText(c)),\n",
    "        #'par_length': get_length(c),\n",
    "        #'doc_length': get_length(doc_text),\n",
    "        #'question_type': get_question_type_features(q),\n",
    "        #'truths': list(set(ans)),\n",
    "        #'q': q,\n",
    "        #'para': c,\n",
    "    #}\n",
    "    #rele =[my_dict]\n",
    "    rele = []\n",
    "    for (_id,title),doc_sim in rets:\n",
    "        content = get_doc_text(str(_id))\n",
    "        if content is None:\n",
    "            continue\n",
    "        paras = content.split('\\n')\n",
    "        for para in paras:\n",
    "            my_dict ={\n",
    "                'doc_id':_id,\n",
    "                'title':title,\n",
    "                'doc_sim': doc_sim,\n",
    "                'par_sim':cossim(q_vec,ret.getTfidfForText(para)),\n",
    "                'par_length':get_length(para),\n",
    "                'doc_length':get_length(content),\n",
    "                'question_type':get_question_type_features(q),\n",
    "                'truths':list(set(ans)),\n",
    "                'q':q,\n",
    "                'para':para\n",
    "            }\n",
    "            rele.append(my_dict)\n",
    "    rele = sorted(rele, key=lambda item: -item[\"par_sim\"])\n",
    "    jsonl.append(rele)\n",
    "    if len(jsonl)==1000:\n",
    "        with open(file_path+f\"/{i}.json\",\"w\") as f:\n",
    "            json.dump(jsonl,f)\n",
    "        jsonl=[]\n",
    "if len(jsonl)!=0:\n",
    "    with open(file_path+f\"/final.json\",\"w\") as f:\n",
    "        json.dump(jsonl,f)\n",
    "    jsonl=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01eb00268ba45acf1093da18c3d5e2059d3803c4cd1bbb63ccfa8889add21a02"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
