# Wikipedia-Open-QnA

Implementation of the Papers DrQA (Module 1 and Module 2) "Reading Wikipedia to Answer Open-Domain Questions" and RankQA (Module 3) "RankQA: Neural Question Answering with Answer Re-Ranking"

## Team Members

- T H Arjun, 2019111012
- Arvindh A, 2019111010

## Data

As part of the project we indexed the whole Wikipedia data dump, implemented the DrQA Retriever, DrQA Reader and extracted features required to run the RankQA model as described in the paper. All the files required to run the modules, train and the reproduce the results are in the attached [here](https://iiitaphyd-my.sharepoint.com/:f:/g/personal/arjun_thekoot_research_iiit_ac_in/EiuPm6Hem95Aib5XDbNh05wB9XeMBeOBKUzecYuQ3IpWtw?e=XddpwF).

## Instructions to run

- Install the required libraries.
- The paths in files are absolute to the machine where we ran the experiments on. To rerun the experiments you will need to modify the paths.
- All of the content in the uploaded directory are necessary to rerun the experiments.
- We have extracted all the features required for training which was computationally heavy and hence we have uploaded them to the shared directory as well.
- We have also given shell scripts required to extract these from the files if required.

## Directory Structure

- `data` directory contains the SQuAD Dataset. You might need to move things from data directory in the link to the `data` directory and change the paths accordingly.

- `module_1` directory has the implementation of the DrQA retriever.
  - `download.sh` will download the Wikipedia database dump required to run the experiments.
  - `make_db.sh` will invoke the necessary python files to create the SQL database of the Wikipedia database dump.
  - `make_index.py` will create a searchable index of the Wikipedia datavase dump with TF-IDF Vectorization.
  - `make_index.sh` will invoke the necessary files files to create the searchable index.
  - `make_tfidf.py` will TF-IDF Vectorize the Wikipedia database dump using multiprocessing.
  - `retriever.py` has the implementation of the DrQA retriever class which works with the files generated by the scripts above.
  - `run_squad.py` file can be used to generate the features required for RankQA from SQuAD Dataset.
  - `retriever_example.ipynb` demonstrates an example on how to use the retriever.

- `module_2` directory contains the implementation of DrQA Reader as described in the paper.
  - `models` directory should be created to save the model checkpoints. The best checkpoint from our experiments is uploaded on the directory shared above.
  - Various python files include the various layers, datasets and other utilities required to train the pytorch model.
  - `drqa.py` has the DrQA class related to the model. `layers.py` , `dictionary.py` , `utils.py` have utilies required to train them.
  - `train.py` can be invoked by `train.sh` and make the necessary path changes to train the DrQA Reader from scratch.
  - `extract.ipynb` can be used along with `make_df.ipynb` for extracting the features required for RankQA.
  - `plot.ipynb` plots various parameters related to training from the report.
  - `test.ipynb` shows how to use DrQA Reader for predictions.

- `module_3` has the implementation of RankQA model.
  - `images` directory contains the images from the report.
  - `convert.ipynb` converts the CSV files from previous steps to JSONL format for easy processing by the RankQA model.
  - `RankQA.ipynb` has the overall implementation of RankQA. The notebook can be run after getting all the necessary files from the shared directory and after changing the absolute paths.

- `paper.pdf` is the annotated PDF version of the given paper.
- `Report.pdf` is the whole project report.

## References

- Kratzwald, B., Eigenmann, A., & Feuerriegel, S. (2019). Rankqa: Neural question answering with answer re-ranking. arXiv preprint arXiv:1906.03008.
- Chen, D., Fisch, A., Weston, J., & Bordes, A. (2017). Reading Wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051.
- Burges, Chris, et al. "Learning to rank using gradient descent." Proceedings of the 22nd international conference on Machine learning. ACM, 2005
