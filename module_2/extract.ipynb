{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"/scratch/arjunth2001/train_df.csv\")#.head(500)\n",
    "df2 = pd.read_csv(\"/scratch/arjunth2001/dev_df.csv\")#.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1= {\n",
    "    'question_ids': [],\n",
    "    'questions': [],\n",
    "    'contexts' : [],\n",
    "    'context_ids': []\n",
    "}\n",
    "dataset2 = {\n",
    "    'question_ids': [],\n",
    "    'questions': [],\n",
    "    'contexts': [],\n",
    "    'context_ids': []\n",
    "}\n",
    "for i in range(len(df1)):\n",
    "    dataset1['question_ids'].append(i)\n",
    "    dataset1['questions'].append(df1['q'][i])\n",
    "    dataset1['contexts'].append(df1['para'][i])\n",
    "    dataset1['context_ids'].append(i)\n",
    "for i in range(len(df2)):\n",
    "    dataset2['question_ids'].append(i)\n",
    "    dataset2['questions'].append(df2['q'][i])\n",
    "    dataset2['contexts'].append(df2['para'][i])\n",
    "    dataset2['context_ids'].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from dotdict import DotDictify\n",
    "from dictionary import Dictionary\n",
    "from tokenizer import SpacyTokenizer\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "import os\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "TOK = None\n",
    "# Each process has its own tokenizer\n",
    "\n",
    "\n",
    "def init_tokenizer(annotators):\n",
    "    global TOK\n",
    "    TOK = SpacyTokenizer(annotators=annotators)\n",
    "\n",
    "# Multiprocessing requires global function\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    global TOK\n",
    "    return TOK.tokenize(text)\n",
    "\n",
    "\n",
    "# def tokenize_all(texts, annotators, num_workers=None):\n",
    "#     \"\"\"Tokenization might take a long time, even when done in parallel\"\"\"\n",
    "#     init_tokenizer(annotators)\n",
    "#     tokens = map(tokenize, texts)\n",
    "#     return list(tokens)\n",
    "def tokenize_all(texts, annotators, num_workers=None):\n",
    "    \"\"\"Tokenization might take a long time, even when done in parallel\"\"\"\n",
    "    workers = Pool(num_workers, init_tokenizer,\n",
    "                   initargs=[annotators])\n",
    "    tokens = workers.map(tokenize, texts)\n",
    "    workers.close()\n",
    "    workers.join()\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(dataset,contexts,questions):\n",
    "    examples = []\n",
    "    for qid, cid in tqdm(enumerate(dataset['context_ids'])):\n",
    "        examples.append({\n",
    "            'id': dataset['question_ids'][qid],\n",
    "            'question': {key: questions[qid][key] for key in ['tokens', 'lemma']},\n",
    "            'context_id': cid,\n",
    "            'answers': {'spans': [], 'texts': []}\n",
    "        })\n",
    "    output = {'contexts': contexts, 'examples': examples}\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    questions1 = tokenize_all(dataset1['questions'], [\n",
    "        'lemma'],12)\n",
    "    contexts1 = tokenize_all(dataset1['contexts'], [\n",
    "        'lemma', 'pos', 'ner'],12)\n",
    "    questions2 = tokenize_all(dataset2['questions'], [\n",
    "        'lemma'], 12)\n",
    "    contexts2 = tokenize_all(dataset2['contexts'], [\n",
    "        'lemma', 'pos', 'ner'], 12)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fc1bed8d1946b4904c508e918d9994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e343b012eede4413b24669532a0e12a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output1 = get_output(dataset1,contexts1,questions1)\n",
    "output2 = get_output(dataset2,contexts2,questions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import drqa\n",
    "import utils\n",
    "from dictionary import Dictionary\n",
    "from dataset import ReadingDataset, BatchSampler\n",
    "from dotdict import DotDictify\n",
    "args = {\n",
    "    \"seed\": 42,\n",
    "    \"data\": \"/scratch/arjunth2001/data\",\n",
    "    \"max_tokens\": 16000,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "    \"max_epoch\": 400,\n",
    "    \"clip_norm\": 10,\n",
    "    \"lr\": 2e-6,\n",
    "    \"momentum\": 0.99,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"lr_shrink\": 0.1,\n",
    "    \"min_lr\": 1e-8,\n",
    "    \"log_file\": \"/scratch/arjunth2001/logs/train.log\",\n",
    "    \"tune_embed\": 1000,\n",
    "    \"checkpoint_dir\": \"./models\",\n",
    "    'embed_dim': 300,\n",
    "    'embed_path': '/scratch/arjunth2001/data/glove.840B.300d.txt',\n",
    "    'hidden_size': 128,\n",
    "    'context_layers': 3,\n",
    "    'question_layers': 3,\n",
    "    'dropout': 0.4,\n",
    "    'bidirectional': True,\n",
    "    'concat_layers': True,\n",
    "    'question_embed': True,\n",
    "    'use_in_question': True,\n",
    "    'use_lemma': True,\n",
    "    'use_pos': True,\n",
    "    'use_ner': True,\n",
    "    'use_tf': True,\n",
    "\n",
    "}\n",
    "args = DotDictify(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Loaded a word dictionary with 29934 words\n",
      "Loading Embedding..\n",
      "Loaded 29547 / 29934 word embeddings (98.71%)\n",
      "Built a model with 13096101 parameters\n",
      "Loaded Checkpoint\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('device: ' + str(device))\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Load a dictionary\n",
    "dictionary = Dictionary.load(os.path.join(args.data, 'dict.txt'))\n",
    "print(\n",
    "    'Loaded a word dictionary with {} words'.format(len(dictionary)))\n",
    "\n",
    "# Load a training and validation dataset\n",
    "with open(os.path.join(args.data, 'train.json')) as file:\n",
    "    train_contents = json.load(file)\n",
    "    train_dataset = ReadingDataset(\n",
    "        args, train_contents['contexts'], train_contents['examples'], dictionary, skip_no_answer=True, single_answer=True)\n",
    "\n",
    "with open(os.path.join(args.data, 'dev.json')) as file:\n",
    "    contents = json.load(file)\n",
    "    valid_dataset = ReadingDataset(\n",
    "        args, contents['contexts'], contents['examples'], dictionary, feature_dict=train_dataset.feature_dict, skip_no_answer=True, single_answer=True\n",
    "    )\n",
    "model = drqa.DrQA.build_model(args, dictionary).to(device)\n",
    "print('Built a model with {} parameters'.format(\n",
    "    sum(p.numel() for p in model.parameters())))\n",
    "# Build an optimizer and a learning rate schedule\n",
    "optimizer = torch.optim.Adamax(\n",
    "    model.parameters(), args.lr, weight_decay=args.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', patience=3, factor=args.lr_shrink)\n",
    "\n",
    "# Load last checkpoint if one exists\n",
    "_ = utils.load_checkpoint(args, model, optimizer, lr_scheduler, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents1 = output1\n",
    "test_dataset1 = ReadingDataset(\n",
    "    args, contents1['contexts'], contents1['examples'], dictionary, feature_dict=train_dataset.feature_dict, skip_no_answer=False, single_answer=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents2 = output2\n",
    "test_dataset2 = ReadingDataset(\n",
    "    args, contents2['contexts'], contents2['examples'], dictionary, feature_dict=train_dataset.feature_dict, skip_no_answer=False, single_answer=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model,test_dataset,df):\n",
    "    scores_ = []\n",
    "    answers_ = []\n",
    "    indxs =[]\n",
    "    ids = []\n",
    "    model.eval()\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, num_workers=args.num_workers, collate_fn=test_dataset.collater, shuffle=False,\n",
    "        batch_sampler=BatchSampler(\n",
    "            test_dataset, args.max_tokens, args.batch_size, shuffle=False, seed=args.seed)\n",
    "    )\n",
    "    progress_bar = tqdm(\n",
    "        test_loader, desc='Testing', leave=False)\n",
    "\n",
    "    for batch_id, sample in enumerate(progress_bar):\n",
    "        sample = utils.move_to_device(sample, device)\n",
    "        with torch.no_grad():\n",
    "            start_scores, end_scores = model(\n",
    "                sample['context_tokens'], sample['question_tokens'],\n",
    "                context_features=sample['context_features']\n",
    "            )\n",
    "\n",
    "            start_pred, end_pred, scores = model.decode(\n",
    "                start_scores, end_scores, max_len=15)\n",
    "\n",
    "            for i, (start_ex, end_ex, score) in enumerate(zip(start_pred, end_pred, scores)):\n",
    "                context = test_dataset.contexts[test_dataset.context_ids[sample['id'][i]]]\n",
    "                start_idx = context['offsets'][start_ex][0]\n",
    "                end_idx = context['offsets'][end_ex][1]\n",
    "                text_pred = context['text'][start_idx: end_idx]\n",
    "                ids.append(sample['id'][i])\n",
    "                answers_.append(text_pred)\n",
    "                scores_.append(score.item())\n",
    "                indxs.append((start_idx,end_idx))\n",
    "    temp = pd.DataFrame()\n",
    "    temp['answer'] = answers_\n",
    "    temp['score'] = scores_\n",
    "    temp['idxs'] = indxs\n",
    "    temp[\"ids\"] = ids\n",
    "    temp.sort_values(by=['ids'],inplace=True)\n",
    "    temp.reset_index(drop=True,inplace=True)\n",
    "    df[\"answer\"] = temp[\"answer\"]\n",
    "    df[\"score\"] = temp[\"score\"]\n",
    "    df[\"idxs\"] = temp[\"idxs\"]\n",
    "    df[\"ids\"] = temp[\"ids\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/16 [00:00<?, ?it/s]/home2/arjunth2001/Wikipedia-Open-QnA/module_2/drqa.py:114: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  pred_start.append(max_idx // scores.size(0))\n"
     ]
    }
   ],
   "source": [
    "df1 = run(model,test_dataset1,df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"/scratch/arjunth2001/output1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df2 = run(model, test_dataset2, df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"/scratch/arjunth2001/output2.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18ad1a0b78abb685195a187077c0e4222d62dc0b12b6ffc1ffdc6633194f7113"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
